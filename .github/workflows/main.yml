name: Smart Pivot Cache Extract (Full Decode + Export XLSX & Parquet + Cleanup)

on:
  workflow_dispatch:

jobs:
  extract:
    runs-on: ubuntu-latest

    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11

      - name: Install dependencies
        run: |
          pip install pandas pyarrow openpyxl lxml requests

      - name: Run Full Smart Pivot Extract + Export XLSX + Parquet + Cleanup
        env:
          TENANT_ID: ${{ secrets.TENANT_ID }}
          CLIENT_ID: ${{ secrets.CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
          USER_EMAIL: "tiennm@tuanvietc5.id.vn"
          SRC_FOLDER: "/1.Job/2425/1. Tracking/Pivot_IDS"
          OUT_XLSX_FOLDER: "/1.Job/2425/1. Tracking/Pivot_To_Month"
          OUT_PARQUET_FOLDER: "/1.Job/2425/1. Tracking/Pivot_Month_Par"
        run: |
          python - <<'PYCODE'
          import os, io, zipfile, requests, pandas as pd
          from lxml import etree

          TENANT_ID = os.environ["TENANT_ID"]
          CLIENT_ID = os.environ["CLIENT_ID"]
          CLIENT_SECRET = os.environ["CLIENT_SECRET"]
          USER_EMAIL = os.environ["USER_EMAIL"]
          SRC_FOLDER = os.environ["SRC_FOLDER"]
          OUT_XLSX_FOLDER = os.environ["OUT_XLSX_FOLDER"]
          OUT_PARQUET_FOLDER = os.environ["OUT_PARQUET_FOLDER"]
          GRAPH_API = "https://graph.microsoft.com/v1.0"

          print("ðŸ” Láº¥y access token ...")
          token_url = f"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token"
          token_data = {
              "grant_type": "client_credentials",
              "client_id": CLIENT_ID,
              "client_secret": CLIENT_SECRET,
              "scope": "https://graph.microsoft.com/.default",
          }
          token_res = requests.post(token_url, data=token_data)
          if not token_res.ok:
              print("âŒ KhÃ´ng láº¥y Ä‘Æ°á»£c token:", token_res.text)
              exit(1)
          token = token_res.json().get("access_token")
          headers = {"Authorization": f"Bearer {token}"}

          # ===== Láº¥y Drive ID =====
          drive_info = requests.get(f"{GRAPH_API}/users/{USER_EMAIL}/drive", headers=headers).json()
          if "id" not in drive_info:
              print("âŒ KhÃ´ng láº¥y Ä‘Æ°á»£c Drive ID:", drive_info)
              exit(1)
          drive_id = drive_info["id"]
          print("ðŸ“ Drive ID:", drive_id)

          # ===== Láº¥y danh sÃ¡ch file trong thÆ° má»¥c nguá»“n =====
          encoded_path = requests.utils.quote(SRC_FOLDER)
          list_url = f"{GRAPH_API}/drives/{drive_id}/root:{encoded_path}:/children"
          res = requests.get(list_url, headers=headers)
          if not res.ok:
              print("âŒ Lá»—i láº¥y danh sÃ¡ch:", res.text)
              exit(1)

          items = res.json().get("value", [])
          xlsx_files = [i for i in items if i["name"].lower().endswith(".xlsx")]
          print(f"ðŸ”Ž TÃ¬m tháº¥y {len(xlsx_files)} file trong {SRC_FOLDER}")

          processed_files = []  # ðŸ”¹ LÆ°u danh sÃ¡ch file xá»­ lÃ½ thÃ nh cÃ´ng

          for idx, f in enumerate(xlsx_files, start=1):
              name = f["name"]
              file_id = f["id"]
              print(f"\nðŸ“¥ Äang xá»­ lÃ½: {name}")

              dl_url = f"{GRAPH_API}/drives/{drive_id}/items/{file_id}/content"
              resp = requests.get(dl_url, headers=headers)
              if resp.status_code != 200:
                  print(f"âŒ Lá»—i táº£i file {name}: {resp.status_code}")
                  continue
              excel_bytes = resp.content

              if excel_bytes[:4] != b'PK\x03\x04':
                  print(f"âš ï¸ {name} khÃ´ng pháº£i Excel há»£p lá»‡.")
                  continue

              try:
                  with zipfile.ZipFile(io.BytesIO(excel_bytes)) as z:
                      record_files = [p for p in z.namelist() if "pivotCacheRecords" in p]
                      if not record_files:
                          print("âš ï¸ KhÃ´ng cÃ³ pivotCacheRecords trong file.")
                          continue

                      for rec in record_files:
                          print(f"ðŸ” Bung {rec} ...")

                          def_path = rec.replace("Records", "Definition")
                          if def_path not in z.namelist():
                              print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y Definition tÆ°Æ¡ng á»©ng.")
                              continue

                          def_xml = etree.fromstring(z.read(def_path))
                          ns = {"d": "http://schemas.openxmlformats.org/spreadsheetml/2006/main"}

                          # ===== Láº¥y sharedItems + tÃªn cá»™t =====
                          fields = []
                          shared_lookup = []
                          for fnode in def_xml.findall(".//d:cacheField", ns):
                              name_field = fnode.get("name") or fnode.get("caption") or "Field"
                              shared = []
                              si = fnode.find(".//d:sharedItems", ns)
                              if si is not None:
                                  for item in si:
                                      tag = etree.QName(item).localname
                                      val = None
                                      if tag == "s":
                                          val = item.get("v") or item.text
                                      elif tag == "n":
                                          try:
                                              val = float(item.get("v") or item.text or 0)
                                          except:
                                              val = item.get("v") or item.text
                                      elif tag == "b":
                                          val = "TRUE" if item.get("v") == "1" else "FALSE"
                                      elif tag == "d":  # âœ… xá»­ lÃ½ ngÃ y thÃ¡ng
                                          val = item.get("v") or item.text
                                      shared.append(val)
                              shared_lookup.append(shared)
                              fields.append(name_field)

                          # ===== Äá»c pivotCacheRecords =====
                          root = etree.fromstring(z.read(rec))
                          rows = []
                          for row in root.findall(".//d:r", ns):
                              row_values = []
                              for i, cell in enumerate(row):
                                  tag = etree.QName(cell).localname
                                  v_attr = cell.get("v")
                                  val = None
                                  if tag == "x" and v_attr is not None:
                                      try:
                                          idx_val = int(v_attr)
                                          if i < len(shared_lookup) and idx_val < len(shared_lookup[i]):
                                              val = shared_lookup[i][idx_val]
                                          else:
                                              val = idx_val
                                      except:
                                          val = v_attr
                                  elif tag == "s":
                                      val = v_attr or cell.text
                                  elif tag == "n":
                                      try:
                                          val = float(v_attr or cell.text or 0)
                                      except:
                                          val = v_attr
                                  elif tag == "b":
                                      val = "TRUE" if (v_attr == "1" or cell.text == "1") else "FALSE"
                                  row_values.append(val)
                              rows.append(row_values)

                          if not rows:
                              print("âš ï¸ Cache rá»—ng.")
                              continue

                          df = pd.DataFrame(rows, columns=fields)

                          # âœ… Chuyá»ƒn cÃ¡c cá»™t cÃ³ tÃªn chá»©a 'date' thÃ nh datetime
                          for col in df.columns:
                              if "date" in col.lower():
                                  df[col] = pd.to_datetime(df[col], errors="ignore")

                          print(f"âœ… Bung Ä‘Æ°á»£c {len(df)} dÃ²ng tá»« {name}")

                          # ===== ðŸ”„ Quy táº¯c Ä‘áº·t tÃªn má»›i (Detail_01, Detail_02, ...) =====
                          file_index = str(idx).zfill(2)
                          out_parquet = f"Detail_{file_index}.parquet"
                          out_xlsx = f"Detail_{file_index}.xlsx"

                          # ===== Ghi file Parquet =====
                          buf_par = io.BytesIO()
                          df.to_parquet(buf_par, index=False)
                          buf_par.seek(0)
                          upload_parquet_url = f"{GRAPH_API}/drives/{drive_id}/root:{OUT_PARQUET_FOLDER}/{out_parquet}:/content?@microsoft.graph.conflictBehavior=replace"
                          up = requests.put(upload_parquet_url, headers=headers, data=buf_par)
                          if up.ok:
                              print(f"ðŸ“¤ ÄÃ£ ghi {out_parquet} vÃ o Pivot_Month_Par (overwrite).")
                          else:
                              print(f"âŒ Lá»—i ghi {out_parquet}:", up.text)
                              continue

                          # ===== Ghi file XLSX =====
                          buf_xlsx = io.BytesIO()
                          with pd.ExcelWriter(buf_xlsx, engine="openpyxl") as writer:
                              df.to_excel(writer, index=False, sheet_name="Detail")
                          buf_xlsx.seek(0)
                          upload_xlsx_url = f"{GRAPH_API}/drives/{drive_id}/root:{OUT_XLSX_FOLDER}/{out_xlsx}:/content?@microsoft.graph.conflictBehavior=replace"
                          upx = requests.put(upload_xlsx_url, headers=headers, data=buf_xlsx)
                          if upx.ok:
                              print(f"ðŸ“— ÄÃ£ ghi {out_xlsx} vÃ o Pivot_To_Month (sheet Detail).")
                              processed_files.append({"id": file_id, "name": name})
                          else:
                              print(f"âŒ Lá»—i ghi {out_xlsx}:", upx.text)

              except Exception as e:
                  print(f"âŒ Lá»—i khi xá»­ lÃ½ {name}: {e}")

          # ===== ðŸ§¹ Dá»n dáº¹p file Ä‘Ã£ xá»­ lÃ½ thÃ nh cÃ´ng =====
          if processed_files:
              print("\nðŸ§¹ Äang xÃ³a cÃ¡c file Ä‘Ã£ xá»­ lÃ½ thÃ nh cÃ´ng trong Pivot_IDS ...")
              for f in processed_files:
                  try:
                      del_url = f"{GRAPH_API}/drives/{drive_id}/items/{f['id']}"
                      del_res = requests.delete(del_url, headers=headers)
                      if del_res.status_code in (200, 204):
                          print(f"ðŸ—‘ï¸ ÄÃ£ xÃ³a: {f['name']}")
                      else:
                          print(f"âš ï¸ Lá»—i khi xÃ³a {f['name']}: {del_res.status_code} - {del_res.text}")
                  except Exception as e:
                      print(f"âŒ Lá»—i khi cá»‘ xÃ³a {f['name']}: {e}")
          else:
              print("\nðŸ“ KhÃ´ng cÃ³ file nÃ o Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»ƒ xÃ³a.")

          print("\nâœ… HoÃ n táº¥t! ÄÃ£ bung cache, ghi ra thÆ° má»¥c Ä‘Ã­ch vÃ  dá»n dáº¹p Pivot_IDS.")
          PYCODE
