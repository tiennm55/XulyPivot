name: Smart Pivot Cache Extract (Full Decode + Export CSV & Parquet + Cleanup)

on:
  workflow_dispatch:

jobs:
  extract:
    runs-on: ubuntu-latest

    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11

      - name: Install dependencies
        run: |
          pip install pandas pyarrow openpyxl lxml requests

      - name: Run Full Smart Pivot Extract + Export CSV + Parquet + Cleanup
        env:
          TENANT_ID: ${{ secrets.TENANT_ID }}
          CLIENT_ID: ${{ secrets.CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
          USER_EMAIL: "tiennm@tuanvietc5.id.vn"
          SRC_FOLDER: "/1.Job/2425/1. Tracking/Pivot_IDS"
          # V·∫´n d√πng bi·∫øn m√¥i tr∆∞·ªùng n√†y l√†m th∆∞ m·ª•c ƒë√≠ch, nh∆∞ng s·∫Ω ch·ª©a file CSV
          OUT_CSV_FOLDER: "/1.Job/2425/1. Tracking/Pivot_To_Month"
          OUT_PARQUET_FOLDER: "/1.Job/2425/1. Tracking/Pivot_Month_Par"
        run: |
          python - <<'PYCODE'
          import os, io, zipfile, requests, pandas as pd
          from lxml import etree

          TENANT_ID = os.environ["TENANT_ID"]
          CLIENT_ID = os.environ["CLIENT_ID"]
          CLIENT_SECRET = os.environ["CLIENT_SECRET"]
          USER_EMAIL = os.environ["USER_EMAIL"]
          SRC_FOLDER = os.environ["SRC_FOLDER"]
          OUT_CSV_FOLDER = os.environ["OUT_CSV_FOLDER"] # ƒê·ªïi t√™n bi·∫øn n·ªôi b·ªô cho d·ªÖ hi·ªÉu
          OUT_PARQUET_FOLDER = os.environ["OUT_PARQUET_FOLDER"]
          GRAPH_API = "https://graph.microsoft.com/v1.0"

          print("üîê L·∫•y access token ...")
          token_url = f"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token"
          token_data = {
              "grant_type": "client_credentials",
              "client_id": CLIENT_ID,
              "client_secret": CLIENT_SECRET,
              "scope": "https://graph.microsoft.com/.default",
          }
          token_res = requests.post(token_url, data=token_data)
          if not token_res.ok:
              print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c token:", token_res.text)
              exit(1)
          token = token_res.json().get("access_token")
          headers = {"Authorization": f"Bearer {token}"}

          # ===== L·∫•y Drive ID =====
          drive_info = requests.get(f"{GRAPH_API}/users/{USER_EMAIL}/drive", headers=headers).json()
          if "id" not in drive_info:
              print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c Drive ID:", drive_info)
              exit(1)
          drive_id = drive_info["id"]
          print("üìÅ Drive ID:", drive_id)

          # ===== L·∫•y danh s√°ch file trong th∆∞ m·ª•c ngu·ªìn =====
          encoded_path = requests.utils.quote(SRC_FOLDER)
          list_url = f"{GRAPH_API}/drives/{drive_id}/root:{encoded_path}:/children"
          res = requests.get(list_url, headers=headers)
          if not res.ok:
              print("‚ùå L·ªói l·∫•y danh s√°ch:", res.text)
              exit(1)

          items = res.json().get("value", [])
          xlsx_files = [i for i in items if i["name"].lower().endswith(".xlsx")]
          print(f"üîé T√¨m th·∫•y {len(xlsx_files)} file trong {SRC_FOLDER}")

          processed_files = []  # üîπ L∆∞u danh s√°ch file x·ª≠ l√Ω th√†nh c√¥ng

          for idx, f in enumerate(xlsx_files, start=1):
              name = f["name"]
              file_id = f["id"]
              print(f"\nüì• ƒêang x·ª≠ l√Ω: {name}")

              dl_url = f"{GRAPH_API}/drives/{drive_id}/items/{file_id}/content"
              resp = requests.get(dl_url, headers=headers)
              if resp.status_code != 200:
                  print(f"‚ùå L·ªói t·∫£i file {name}: {resp.status_code}")
                  continue
              excel_bytes = resp.content

              if excel_bytes[:4] != b'PK\x03\x04':
                  print(f"‚ö†Ô∏è {name} kh√¥ng ph·∫£i Excel h·ª£p l·ªá.")
                  continue

              try:
                  with zipfile.ZipFile(io.BytesIO(excel_bytes)) as z:
                      record_files = [p for p in z.namelist() if "pivotCacheRecords" in p]
                      if not record_files:
                          print("‚ö†Ô∏è Kh√¥ng c√≥ pivotCacheRecords trong file.")
                          continue

                      for rec in record_files:
                          print(f"üîç Bung {rec} ...")

                          def_path = rec.replace("Records", "Definition")
                          if def_path not in z.namelist():
                              print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Definition t∆∞∆°ng ·ª©ng.")
                              continue

                          def_xml = etree.fromstring(z.read(def_path))
                          ns = {"d": "http://schemas.openxmlformats.org/spreadsheetml/2006/main"}

                          # ===== L·∫•y sharedItems + t√™n c·ªôt =====
                          fields = []
                          shared_lookup = []
                          for fnode in def_xml.findall(".//d:cacheField", ns):
                              name_field = fnode.get("name") or fnode.get("caption") or "Field"
                              shared = []
                              si = fnode.find(".//d:sharedItems", ns)
                              if si is not None:
                                  for item in si:
                                      tag = etree.QName(item).localname
                                      val = None
                                      if tag == "s":
                                          val = item.get("v") or item.text
                                      elif tag == "n":
                                          try:
                                              val = float(item.get("v") or item.text or 0)
                                          except:
                                              val = item.get("v") or item.text
                                      elif tag == "b":
                                          val = "TRUE" if item.get("v") == "1" else "FALSE"
                                      elif tag == "d":
                                          val = item.get("v") or item.text
                                      shared.append(val)
                              shared_lookup.append(shared)
                              fields.append(name_field)

                          # ===== ƒê·ªçc pivotCacheRecords =====
                          root = etree.fromstring(z.read(rec))
                          rows = []
                          for row in root.findall(".//d:r", ns):
                              row_values = []
                              for i, cell in enumerate(row):
                                  tag = etree.QName(cell).localname
                                  v_attr = cell.get("v")
                                  val = None
                                  if tag == "x" and v_attr is not None:
                                      try:
                                          idx_val = int(v_attr)
                                          if i < len(shared_lookup) and idx_val < len(shared_lookup[i]):
                                              val = shared_lookup[i][idx_val]
                                          else:
                                              val = idx_val
                                      except:
                                          val = v_attr
                                  elif tag == "s":
                                      val = v_attr or cell.text
                                  elif tag == "n":
                                      try:
                                          val = float(v_attr or cell.text or 0)
                                      except:
                                          val = v_attr
                                  elif tag == "b":
                                      val = "TRUE" if (v_attr == "1" or cell.text == "1") else "FALSE"
                                  row_values.append(val)
                              rows.append(row_values)

                          if not rows:
                              print("‚ö†Ô∏è Cache r·ªóng.")
                              continue

                          # ===== T·∫°o DataFrame =====
                          df = pd.DataFrame(rows, columns=fields)
                          print(f"‚úÖ Bung ƒë∆∞·ª£c {len(df)} d√≤ng t·ª´ {name}")

                          # ===== ƒê·∫∑t t√™n file ƒë·∫ßu ra =====
                          file_index = str(idx).zfill(2)
                          out_parquet = f"Detail_{file_index}.parquet"
                          out_csv = f"Detail_{file_index}.csv"  # üîπ ƒê·ªïi ƒëu√¥i th√†nh CSV

                          # ===== Ghi file Parquet =====
                          buf_par = io.BytesIO()
                          df.to_parquet(buf_par, index=False)
                          buf_par.seek(0)
                          upload_parquet_url = f"{GRAPH_API}/drives/{drive_id}/root:{OUT_PARQUET_FOLDER}/{out_parquet}:/content?@microsoft.graph.conflictBehavior=replace"
                          up = requests.put(upload_parquet_url, headers=headers, data=buf_par)
                          if up.ok:
                              print(f"üì§ ƒê√£ ghi {out_parquet} v√†o Pivot_Month_Par (overwrite).")
                          else:
                              print(f"‚ùå L·ªói ghi {out_parquet}:", up.text)
                              continue

                          # ===== Ghi file CSV (Thay th·∫ø ƒëo·∫°n XLSX c≈©) =====
                          # 1. Chuy·ªÉn DataFrame th√†nh String CSV
                          # 2. Encode sang bytes v·ªõi utf-8-sig ƒë·ªÉ ƒë·ªçc ƒë∆∞·ª£c ti·∫øng Vi·ªát
                          csv_string = df.to_csv(index=False, encoding='utf-8-sig')
                          csv_bytes = csv_string.encode('utf-8-sig')

                          upload_csv_url = f"{GRAPH_API}/drives/{drive_id}/root:{OUT_CSV_FOLDER}/{out_csv}:/content?@microsoft.graph.conflictBehavior=replace"
                          upx = requests.put(upload_csv_url, headers=headers, data=csv_bytes)
                          
                          if upx.ok:
                              print(f"üìó ƒê√£ ghi {out_csv} v√†o Pivot_To_Month (CSV).")
                              processed_files.append({"id": file_id, "name": name})
                          else:
                              print(f"‚ùå L·ªói ghi {out_csv}:", upx.text)

              except Exception as e:
                  print(f"‚ùå L·ªói khi x·ª≠ l√Ω {name}: {e}")

          # ===== üßπ D·ªçn d·∫πp file ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng =====
          if processed_files:
              print("\nüßπ ƒêang x√≥a c√°c file ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng trong Pivot_IDS ...")
              for f in processed_files:
                  try:
                      del_url = f"{GRAPH_API}/drives/{drive_id}/items/{f['id']}"
                      del_res = requests.delete(del_url, headers=headers)
                      if del_res.status_code in (200, 204):
                          print(f"üóëÔ∏è ƒê√£ x√≥a: {f['name']}")
                      else:
                          print(f"‚ö†Ô∏è L·ªói khi x√≥a {f['name']}: {del_res.status_code} - {del_res.text}")
                  except Exception as e:
                      print(f"‚ùå L·ªói khi c·ªë x√≥a {f['name']}: {e}")
          else:
              print("\nüìÅ Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªÉ x√≥a.")

          print("\n‚úÖ Ho√†n t·∫•t! ƒê√£ bung cache, ghi ra th∆∞ m·ª•c ƒë√≠ch v√† d·ªçn d·∫πp Pivot_IDS.")
          PYCODE
