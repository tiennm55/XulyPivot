name: Extract Pivot Cache to Excel and Parquet

on:
  workflow_dispatch:

jobs:
  extract:
    runs-on: ubuntu-latest
    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11

      - name: Install dependencies
        run: |
          pip install requests pandas lxml pyarrow openpyxl

      - name: Run Pivot Cache Extractor
        env:
          TENANT_ID: ${{ secrets.TENANT_ID }}
          CLIENT_ID: ${{ secrets.CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
          USER_EMAIL: ${{ secrets.USER_EMAIL }}
          SRC_FOLDER: "/1.Job/2425/1. Tracking/Pivot_Month_Par"
          OUT_XLSX: "/1.Job/2425/1. Tracking/Pivot_Month"
        run: |
          python - <<'PYCODE'
          import io, os, zipfile, urllib.parse, requests, pandas as pd
          from lxml import etree

          # ===============================
          # üîê C·∫§U H√åNH X√ÅC TH·ª∞C
          # ===============================
          TENANT_ID = os.environ["TENANT_ID"]
          CLIENT_ID = os.environ["CLIENT_ID"]
          CLIENT_SECRET = os.environ["CLIENT_SECRET"]
          USER_EMAIL = os.environ["USER_EMAIL"]
          SRC_FOLDER = os.environ["SRC_FOLDER"]
          OUT_XLSX = os.environ["OUT_XLSX"]

          token_url = f"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token"
          token_data = {
              "grant_type": "client_credentials",
              "client_id": CLIENT_ID,
              "client_secret": CLIENT_SECRET,
              "scope": "https://graph.microsoft.com/.default"
          }
          token = requests.post(token_url, data=token_data).json()["access_token"]
          headers = {"Authorization": f"Bearer {token}"}

          # ===============================
          # üìÇ L·∫§Y TH√îNG TIN ONEDRIVE BUSINESS
          # ===============================
          user_drive = requests.get(
              f"https://graph.microsoft.com/v1.0/users/{USER_EMAIL}/drive", headers=headers
          ).json()
          if "id" not in user_drive:
              print("‚ùå Kh√¥ng th·ªÉ l·∫•y Drive ID:", user_drive)
              exit(1)

          drive_id = user_drive["id"]
          print("üìÅ Drive ID:", drive_id)

          # ===============================
          # üß© H√ÄM M√É H√ìA ƒê∆Ø·ªúNG D·∫™N
          # ===============================
          def encode_path(path: str) -> str:
              """M√£ h√≥a URL gi·ªØ nguy√™n d·∫•u '/'"""
              return urllib.parse.quote(path, safe="/")

          src_path_enc = encode_path(SRC_FOLDER)
          out_path_enc = encode_path(OUT_XLSX)

          # ===============================
          # üìã L·∫§Y DANH S√ÅCH FILE NGU·ªíN
          # ===============================
          list_url = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:{src_path_enc}:/children"
          res = requests.get(list_url, headers=headers).json()
          files = res.get("value", [])

          if not files:
              print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file n√†o trong th∆∞ m·ª•c:", SRC_FOLDER)
              print("Response:", res)
              exit()

          print(f"üîé T√¨m th·∫•y {len(files)} file trong th∆∞ m·ª•c ngu·ªìn.")

          # ===============================
          # üß† X·ª¨ L√ù M·ªñI FILE .XLSX
          # ===============================
          for f in files:
              name = f["name"]
              if not name.lower().endswith(".xlsx"):
                  continue

              print(f"\nüì• ƒêang x·ª≠ l√Ω file: {name}")

              file_url = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:{src_path_enc}/{urllib.parse.quote(name)}:/content"
              excel_bytes = requests.get(file_url, headers=headers).content

              try:
                  with zipfile.ZipFile(io.BytesIO(excel_bytes)) as z:
                      # T√¨m c√°c file cache XML
                      record_files = [p for p in z.namelist() if "pivotCacheRecords" in p]
                      if not record_files:
                          print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y pivotCacheRecords trong file.")
                          continue

                      for record_file in record_files:
                          print(f"üîç ƒê·ªçc {record_file} ...")
                          xml_data = z.read(record_file)
                          root = etree.fromstring(xml_data)
                          ns = {"d": "http://schemas.openxmlformats.org/spreadsheetml/2006/main"}

                          rows = []
                          for r in root.findall(".//d:r", namespaces=ns):
                              values = [x.text for x in r.findall(".//d:x", namespaces=ns)]
                              if values:
                                  rows.append(values)

                          if not rows:
                              print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong cache.")
                              continue

                          df = pd.DataFrame(rows)
                          print(f"‚úÖ Bung ƒë∆∞·ª£c {len(df)} d√≤ng d·ªØ li·ªáu.")

                          base_name = name.replace(".xlsx", "")
                          out_xlsx_file = f"{OUT_XLSX}/{base_name}_cache.xlsx"
                          out_parquet_file = f"{SRC_FOLDER}/{base_name}_cache.parquet"

                          # ===============================
                          # üíæ GHI FILE EXCEL
                          # ===============================
                          buf_xlsx = io.BytesIO()
                          df.to_excel(buf_xlsx, index=False)
                          buf_xlsx.seek(0)
                          requests.put(
                              f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:{encode_path(out_xlsx_file)}:/content",
                              headers=headers,
                              data=buf_xlsx
                          )

                          # ===============================
                          # üíæ GHI FILE PARQUET
                          # ===============================
                          buf_parq = io.BytesIO()
                          df.to_parquet(buf_parq, index=False)
                          buf_parq.seek(0)
                          requests.put(
                              f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:{encode_path(out_parquet_file)}:/content",
                              headers=headers,
                              data=buf_parq
                          )

                          print(f"üì§ ƒê√£ ghi {base_name}_cache.xlsx v√† {base_name}_cache.parquet th√†nh c√¥ng!")

              except Exception as e:
                  print(f"‚ùå L·ªói khi x·ª≠ l√Ω {name}: {e}")

          print("\nüèÅ Ho√†n t·∫•t bung to√†n b·ªô Pivot Cache.")
          PYCODE
